% ---
% RESUMOS
% ---

% RESUMO em português
\setlength{\absparsep}{18pt} % ajusta o espaçamento dos parágrafos do resumo
\begin{resumo}

O aumento na disseminação de desinformação, juntamente com sua rápida propagação por meio de
plataformas digitais, tornou-se uma preocupação significativa para a sociedade moderna,
ressaltando a necessidade de lidar com esse problema. Modelos de classificação de textos,
utilizando aprendizado de máquina e aprendizado profundo, surgem como alternativas
promissoras para enfrentar essa questão. O método de aprendizado de máquina tradicional
possui o conteúdo textual representado por um conjunto de características, lexicais, sintáticas,
semânticas e discursivas, selecionadas manualmente. Classificadores supervisionados, como por
exemplo, SVMs, têm sido utilizados para realizar o agrupamento de diferentes notícias. Por
outro lado, modelos de aprendizado profundo, apresentam o conteúdo textual frequentemente
como uma matriz que é interpretada por redes neurais treinadas, como por exemplo RNNs, com
funções de extrair características textuais e posteriormente realizar a classificação textual. Além
disso, abordagens baseadas em transformers, como o BERT (Bidirectional Encoder
Representations from Transformers), têm se destacado no processamento de linguagem natural,
permitindo um entendimento mais profundo do contexto e do fluxo da linguagem. Deste modo,
este projeto visa fazer uma análise sistemática para descobrir a eficácia entre os métodos
estudados e a viabilidade para se colocar em prática cada um dos modelos. Além disso, os dados
obtidos neste projeto serão utilizados para auxiliar futuros desafios gerados pela manipulação
maliciosa de dados textuais.


 \textbf{Palavras-chaves}: Desinformação, classificação de texto, aprendizado de máquina.
\end{resumo}

% ABSTRACT in english
\begin{resumo}[Abstract]
 \begin{otherlanguage*}{english}
    The increase in the spread of misinformation, along with its rapid spread through digital platforms, has
become a significant concern for modern society, highlighting the need to address this issue.
Text classification models using machine learning and deep learning have emerged as promising
alternatives to tackle this problem. Traditional machine learning methods represent textual
content as a set of manually selected lexical, syntactic, semantic, and discursive features.
Supervised classifiers, such as SVMs, have been used to group different news articles. On the
other hand, deep learning models often represent textual content as a matrix interpreted by
trained neural networks, such as RNNs, which extract textual features and perform text
classification. Additionally, transformer-based approaches, such as BERT (Bidirectional Encoder
Representations from Transformers), have excelled in natural language processing, allowing for
a deeper understanding of language context and flow. Thus, this project aims to conduct a
systematic analysis to discover the effectiveness of the studied methods and the practicality of
implementing each model. Furthermore, the data obtained in this project will be used to assist in
future challenges arising from the malicious manipulation of textual data.

   \vspace{\onelineskip}
 
   \noindent 
   \textbf{Keywords}: Misinformation, text classification, machine learning.
 \end{otherlanguage*}
\end{resumo}