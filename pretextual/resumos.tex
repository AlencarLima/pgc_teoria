% ---
% RESUMOS
% ---

% RESUMO em português
\setlength{\absparsep}{18pt} % ajusta o espaçamento dos parágrafos do resumo
\begin{resumo}

A crescente disseminação de desinformação em ambientes digitais, aliada à velocidade com que conteúdos circulam em plataformas e redes sociais, transformou-se em um desafio central para a sociedade contemporânea. Nesse contexto, modelos de classificação de textos baseados em aprendizado de máquina e aprendizado profundo despontam como alternativas promissoras para apoiar a detecção e mitigação desse fenômeno. Em abordagens tradicionais de aprendizado de máquina, o texto é representado por conjuntos de características lexicais, sintáticas, semânticas e discursivas selecionadas manualmente, e classificadores supervisionados, como SVMs, têm sido amplamente empregados na separação de diferentes tipos de notícias. Já modelos de aprendizado profundo descrevem o conteúdo textual como matrizes processadas por redes neurais, como RNNs, capazes de extrair automaticamente representações relevantes para posterior classificação. Mais recentemente, arquiteturas baseadas em transformers, como o BERT (Bidirectional Encoder Representations from Transformers), têm se destacado no Processamento de Linguagem Natural por permitir uma modelagem mais precisa do contexto e do fluxo da linguagem. À luz desse cenário, este projeto propõe uma análise sistemática da eficácia e da viabilidade prática de distintos métodos de extração de termos-chave e classificação textual aplicados à detecção de desinformação, buscando identificar vantagens, limitações e potenciais de uso em cenários reais. Os dados obtidos servirão de base para o aperfeiçoamento de ferramentas computacionais voltadas ao enfrentamento de usos maliciosos de dados textuais.


 \textbf{Palavras-chaves}: Desinformação, extração de termos-chave, classificação de texto, aprendizado de máquina.
\end{resumo}

% ABSTRACT in english
\begin{resumo}[Abstract]
 \begin{otherlanguage*}{english}
    The growing spread of disinformation in digital environments, together with the speed at which content circulates on platforms and social networks, has become a central challenge for contemporary society. In this context, text classification models based on machine learning and deep learning have emerged as promising alternatives to support the detection and mitigation of this phenomenon. In traditional machine learning approaches, text is represented by sets of lexical, syntactic, semantic, and discursive features selected manually, and supervised classifiers such as SVMs have been widely used to distinguish between different types of news. Deep learning models, in turn, represent textual content as matrices processed by neural networks, such as RNNs, which are capable of automatically extracting relevant representations for subsequent classification. More recently, transformer-based architectures, such as BERT (Bidirectional Encoder Representations from Transformers), have stood out in Natural Language Processing by enabling a more accurate modeling of linguistic context and flow. In light of this scenario, this project proposes a systematic analysis of the effectiveness and practical feasibility of different methods for key term extraction and text classification applied to disinformation detection, seeking to identify advantages, limitations, and potential for use in real-world settings. The results will serve as a basis for improving computational tools aimed at countering malicious uses of textual data.

   \vspace{\onelineskip}
 
   \noindent 
   \textbf{Keywords}: Disinformation, key term extraction, text classification, machine learning.
 \end{otherlanguage*}
\end{resumo}