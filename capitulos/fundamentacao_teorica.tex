\chapter{Fundamentação Teórica}
\label{cap:fundamentacao_teorica}

O avanço das tecnologias de Processamento de Linguagem Natural (PLN) e de Aprendizado de Máquina (\textit{Machine Learning-ML}) tem transformado profundamente a forma como se compreende, organiza e interpreta grandes volumes de informação textual. Essas áreas, situadas na intersecção entre a Linguística Computacional (LC) e a Inteligência Artificial (IA), fornecem as bases para o desenvolvimento de sistemas capazes de compreender o conteúdo semântico de textos, extrair padrões linguísticos e realizar tarefas de classificação, sumarização e detecção de desinformação. Neste capítulo, são abordados os principais conceitos e fundamentos teóricos que sustentam as técnicas utilizadas nesta pesquisa, com ênfase em conceitos utilizados no projeto, como nas metodologias de extração de termos factuais e nos modelos de classificação textual aplicados ao combate à disseminação de desinformação nos meios digitais.

\section{Desinformação}

A desinformação é um fenômeno multifacetado que se refere à disseminação intencional de informações falsas ou enganosas com o objetivo de manipular percepções e comportamentos \cite{surveyFakeNews}. No ambiente digital, sua propagação é potencializada pela velocidade das redes sociais e pela personalização algorítmica, que cria bolhas informacionais e amplia o impacto de narrativas falsas.

É importante ter em mente o tipo específico de informação que será tratado neste trabalho, uma vez que o fenômeno da desordem informacional representa uma das expressões mais complexas da crise contemporânea da comunicação. Conforme analisa Silva~\cite{silva2020}, a desordem informacional manifesta-se no ambiente digital como um caos comunicacional, em que conteúdos verdadeiros, falsos e distorcidos se misturam em alta velocidade, especialmente nas redes sociais como o \textit{Twitter}. Essa dinâmica, acelerada pela pandemia de COVID-19, caracteriza a chamada infodemia, marcada pelo excesso de informações, algumas corretas, outras enganosas, que dificultam a distinção entre fontes confiáveis e conteúdos manipulados e, ao mesmo tempo, oferecem um campo fértil para a identificação de termos e expressões que sinalizam diferentes tipos de distorção. Como sintetizado na Figura~\ref{fig:desordem_informacional}, essas categorias se sobrepõem e ajudam a diferenciar situações em que há apenas circulação de informação incorreta daquelas em que há uso estratégico da informação para produzir dano.

% Colocar imagem ilustrativa da desordem informacional
\begin{figure}[H] % ou [!htb] se preferir deixar o LaTeX decidir
    \centering
    \includegraphics[width=1\textwidth]{figs/desordem_informacional.png} % coloque o arquivo na pasta escolhida
    \caption{"Desordem da informação".}
    \label{fig:desordem_informacional}
    \legend{\footnotesize Fonte: \cite{unesco2018}} % abnTeX2
\end{figure}

De acordo com a tipologia proposta por Wardle e Derakhshan~\cite{wardle2017}, a desordem informacional se estrutura em três dimensões complementares:
\begin{itemize}
    \item Informação Incorreta: quando informações falsas são compartilhadas sem intenção de causar dano;
    \item Desinformação: quando há intenção deliberada de enganar ou prejudicar;
    \item Má-informação: quando informações verdadeiras são utilizadas com o objetivo de causar dano.
\end{itemize}

Esses três eixos foram incorporados e ampliados no manual da UNESCO~\cite{unesco2018}, intitulado \textit{Journalism, “Fake News” \& Disinformation}, que trata a desordem informacional como um fenômeno sistêmico, envolvendo dimensões políticas, tecnológicas e cognitivas.

Sob essa perspectiva, o Tribunal Superior Eleitoral (TSE) passou a adotar “desinformação” como conceito guarda-chuva para abarcar diferentes formas de manipulação e distorção de conteúdo em contextos de desordem informacional~\cite{tse2022}. Materiais do TRE-SP explicitam que “\textit{fake news}” é uma subcategoria específica, ligada à falsificação da forma notícia e à aparência jornalística, enquanto “desinformação” cobre um escopo mais amplo, incluindo postagens, conteúdos audiovisuais, memes e narrativas distorcidas que circulam nas redes~\cite{tresp2023}. Assim, para estudos sobre circulação de informações \textit{online} e mais especificamente, para este trabalho, o uso do termo “desinformação” mostra-se conceitualmente mais preciso e metodologicamente consistente~\cite{tse2022,tresp2023}.

A literatura especializada reforça essa ampliação conceitual. Marchiori~\cite{marchiori2002} já identificava o paradoxo do excesso informacional, no qual o volume de dados não se converte em conhecimento. Han~\cite{han2018}, por sua vez, descreve o ambiente digital como um “enxame de ruídos”, no qual indivíduos isolados produzem e compartilham incessantemente fragmentos de informação sem coordenação nem filtro, gerando ruído cognitivo coletivo.

Estudos empíricos comprovam os efeitos dessa desordem. Vosoughi, Roy e Aral~\cite{vosoughi2018} mostram que, no \textit{Twitter}, boatos falsos difundem-se mais longe, mais rápido e mais profundamente do que notícias verdadeiras, chegando a 1,500 pessoas cerca de seis vezes mais rápido, em grande parte associados à novidade da informação e a um perfil emocional distinto nas respostas (mais surpresa e nojo), e não ao efeito de \textit{bots}. Cinelli et al.~\cite{cinelli2020} analisam várias plataformas e encontram padrões de difusão semelhantes para conteúdos de fontes confiáveis e questionáveis. No \textit{Twitter}, as estimativas de amplificação indicam que a arquitetura não discrimina a veracidade no ato da difusão, favorecendo a circulação indiscriminada de conteúdo.

Durante a pandemia, o \textit{Covid19 Infodemic Observatory}~\cite{dedomenico2020} verificou que cerca de 28,9\% das publicações sobre COVID-19 continham informações questionáveis. Já a Organização Pan-Americana da Saúde (OPAS)~\cite{opas2020} conceituou a infodemia como uma ``superabundância de informações, precisas e imprecisas, que dificulta a identificação de orientações confiáveis'', destacando o papel das redes sociais como amplificadoras desse processo.

Diante desse cenário, García-Saisó et al.~\cite{garciasaiso2021} enfatizam que a desordem informacional deve ser tratada como questão de saúde pública e de governança democrática, demandando estratégias de comunicação de risco, fortalecimento da checagem de fatos e políticas de alfabetização midiática.

Em síntese, a desordem informacional e, dentro dela, a desinformação, traduz a crise da racionalidade comunicacional contemporânea. Mais do que um problema semântico, trata-se de uma reconfiguração estrutural do ecossistema informativo, em que a abundância de conteúdo, a ausência de filtros e o incentivo algorítmico à polarização substituem a busca pela verdade pela simples viralização. Dessa forma, o presente trabalho se concentra especificamente no estudo da desinformação.

Modelos como o \textit{XLM-R-Large-ClaimDetection}, utilizado neste estudo, aplicam técnicas de aprendizado profundo para identificar automaticamente frases que contêm declarações factuais dignas de verificação. Essa abordagem se mostra promissora ao reduzir a sobrecarga humana e aumentar a escalabilidade de sistemas de monitoramento de desinformação.

\section{Processamento de Linguagem Natural}

O PLN é uma área da Inteligência Artificial dedicada ao desenvolvimento de métodos que possibilitam que computadores compreendam e manipulem a linguagem humana de forma automatizada \cite{jurafsky2023}. O PLN combina aspectos da linguística, estatística e ciência da computação, permitindo o tratamento de dados textuais em larga escala e a extração de informações de maneira sistemática. Entre suas principais tarefas estão a tokenização, remoção de \textit{stopwords}, lematização, \textit{stemming}, análise sintática e semântica, reconhecimento de entidades nomeadas (\textit{NER}) e análise de sentimentos.  

No contexto desta pesquisa, o PLN é utilizado para o pré-processamento de textos, etapa que inclui a normalização textual, a segmentação de sentenças e a identificação de termos relevantes. Essas técnicas garantem que o texto seja representado de maneira estruturada, possibilitando a aplicação de algoritmos de \textit{ML} e aprendizado profundo. A capacidade do PLN de identificar padrões linguísticos e semânticos possui grande relevância para a detecção de desinformação, uma vez que esse fenômeno se manifesta por meio de estruturas discursivas específicas, escolhas lexicais tendenciosas e manipulação intencional de contextos.

\section{Representação de Texto}

A representação de texto constitui um dos pilares do PLN, pois define a forma como as palavras e sentenças são convertidas em formatos compreensíveis para os algoritmos computacionais. Entre os métodos mais tradicionais, destacam-se o modelo de \textit{Bag of Words} (\textit{BoW}) e o \textit{Term Frequency - Inverse Document Frequency} (\textit{TF-IDF}) \cite{salton1975vector}.

O modelo \textit{BoW} representa cada documento como um vetor em $\mathbb{R}^V$, em que $V$ é o tamanho do vocabulário, e cada componente corresponde tipicamente à frequência (ou presença/ausência) de um termo no documento. Nessa representação, assume-se independência entre termos e ignora-se completamente a ordem das palavras, de modo que dois textos com o mesmo multiconjunto de termos terão a mesma representação vetorial \cite{hacochenkerner2020bow}. 

Já o \textit{TF-IDF} aprimora o modelo \textit{BoW} ao ponderar a importância de cada termo com base em sua frequência no documento e na raridade no corpus \cite{manning1999foundations,salton1975vector}. 

Formalmente, seja $f_{ik}$ a frequência do termo $k$ no documento $i$ (com $i = 1,\dots,n$ documentos e vocabulário indexado por $k$). Seguindo o modelo vetorial clássico de Salton, Wong e Yang \cite{salton1975vector}, a componente de frequência de termo (\textit{term frequency}, TF) é dada por
\begin{equation}
    \mathrm{TF}_{ik} = f_{ik}.
\end{equation}

Seja ainda $d_k$ o número de documentos em que o termo $k$ aparece (\textit{document frequency}) e $n$ o número total de documentos da coleção. A componente de frequência inversa de documento (\textit{inverse document frequency}, IDF) pode ser escrita como
\begin{equation}
    \mathrm{IDF}_k = \left\lceil \log_{2}(n) \right\rceil
                   - \left\lceil \log_{2}(d_k) \right\rceil
                   + 1,
\end{equation}

de modo que o peso TF-IDF atribuído ao termo $k$ no documento $i$ é
\begin{equation}
    w_{ik} = \mathrm{TF\text{-}IDF}_{ik} = f_{ik} \cdot \mathrm{IDF}_k.
\end{equation}

Com o avanço da área, surgiram métodos mais sofisticados de representação vetorial, conhecidos como \textit{word embeddings}, que mapeiam palavras em vetores contínuos em um espaço de alta dimensionalidade. Técnicas como \textit{Word2Vec} \cite{mikolov2013distributed}, \textit{GloVe} \cite{pennington2014glove} e \textit{FastText} \cite{bojanowski2017enriching} capturam relações semânticas e contextuais entre palavras com base em sua coocorrência. Os \textit{embeddings} contextuais, derivados de modelos mais complexos como \textit{BERT} e \textit{SBERT}, passaram a representar o significado das palavras considerando o contexto em que aparecem, oferecendo resultados superiores em tarefas semânticas complexas.

\section{Sumarização Extrativa}
A sumarização extrativa de texto é uma técnica de PLN que visa gerar versões
reduzidas de documentos, preservando as informações mais relevantes e essenciais para o entendimento do conteúdo original. Dentro dessa abordagem, as sentenças mais significativas são selecionadas diretamente do texto-fonte, sem a necessidade de reformulação ou geração de novas frases, ou seja, apenas reutiliza trechos existentes no texto original.

A tarefa pode ser vista como um problema de seleção de unidades textuais (tipicamente sentenças) a partir de um documento ou de um conjunto de
documentos, de modo a maximizar a cobertura do conteúdo informativo e minimizar
a redundância, produzindo um sumário curto, coerente e informativo. Segundo
Nenkova e McKeown, a sumarização automática consiste justamente em selecionar
e integrar o conteúdo-chave de um ou mais textos em uma saída condensada, sendo as abordagens extrativas a forma mais tradicional e amplamente estudada
\cite{nenkova2011automatic}.

Do ponto de vista metodológico, pesquisas como a de Gupta e Lehal (2010) mostram que as técnicas extrativas podem ser agrupadas em abordagens baseadas em características superficiais do texto (frequência de termos, posição da sentença), em modelos de aprendizado supervisionado que classificam sentenças
como relevantes ou não e em métodos baseados em grafos, que avaliam a importância de uma sentença pela sua centralidade em uma rede de similaridades
\cite{gupta2010survey}.

Já para o contexto da língua portuguesa, Costa e Martins (2015) comparam sistematicamente diferentes estratégias de sumarização automática extrativa, incluindo abordagens de frequência, centróides e métodos grafo-baseados, e mostram que, mesmo sem geração de novas frases, essas técnicas são capazes de produzir sumários competitivos, desde que combinadas com boas estratégias de seleção e avaliadas por métricas apropriadas \cite{costa2015comparacao}.

Portanto, a sumarização extrativa representa uma ferramenta valiosa para a redução de textos longos, facilitando a compreensão rápida de conteúdos postados em redes sociais ou de notícias, sem perder a essência informativa. No presente estudo, essa técnica foi empregada para identificar e extrair sentenças factuais de notícias, contribuindo para a análise e classificação de informações relevantes no combate à desinformação.

\section{Aprendizado de Máquina na Classificação Textual}

O Aprendizado de Máquina (\textit{ML}) é uma subárea da IA voltada para o desenvolvimento de algoritmos capazes de aprender padrões a partir de dados e realizar previsões ou classificações sem intervenção humana direta \cite{mitchell1997machine}. No contexto do PLN, o \textit{ML} é amplamente utilizado para classificar textos, identificar tópicos, analisar sentimentos e detectar desinformação.  

Os modelos tradicionais de \textit{ML} aplicados a textos incluem algoritmos supervisionados como \textit{Support Vector Machines} (\textit{SVM}), \textit{Naive Bayes} e \textit{Random Forests}. Esses métodos dependem de representações vetoriais do texto, como \textit{BoW} e \textit{TF-IDF}, para extrair características numéricas. O \textit{SVM}, em particular, tem se mostrado eficiente na separação de classes lineares e foi utilizado neste estudo para a classificação de sentenças factuais e não factuais, alcançando resultados satisfatórios em acurácia, embora com limitações quanto à relevância prática das classificações.  

Para a avaliação dos modelos de \textit{ML}, métricas como precisão, \textit{recall}, \textit{F1-score} e acurácia são amplamente utilizadas. A precisão mede a proporção de classificações corretas entre as predições positivas. O \textit{recall} avalia a capacidade do modelo em identificar todas as instâncias relevantes. O \textit{F1-score} combina ambas as métricas, oferecendo uma medida harmônica de desempenho. Tais métricas são fundamentais para compreender a eficiência dos algoritmos de classificação textual aplicados à detecção de desinformação.  

\section{Aprendizado Profundo (\textit{Deep Learning})}

O Aprendizado Profundo (\textit{Deep Learning - DL}) representa uma evolução do \textit{ML}, caracterizando-se pela utilização de redes neurais artificiais com múltiplas camadas de processamento. Essas redes são capazes de modelar relações complexas e não lineares nos dados, tornando-se particularmente eficazes em tarefas envolvendo linguagem natural.  

As Redes Neurais Recorrentes (\textit{RNNs}), especialmente suas variantes \textit{LSTM} (\textit{Long Short-Term Memory}) e \textit{GRU} (\textit{Gated Recurrent Unit}), foram amplamente empregadas para capturar dependências temporais em sequências textuais. No entanto, apesar de seu sucesso, essas arquiteturas apresentam limitações quanto à paralelização e ao tratamento de contextos longos.  

Essas limitações foram superadas com o surgimento dos \textit{Transformers} \cite{vaswani2017attention}, 
uma arquitetura baseada em mecanismos de atenção que permite o aprendizado bidirecional do contexto textual, 
conforme ilustrado na Figura~\ref{fig:diagrama_tranformer}. Modelos derivados, como \textit{BERT} (\textit{Bidirectional Encoder Representations from Transformers}), 
\textit{DistilBERT}, \textit{SBERT} (\textit{Sentence-BERT}) e \textit{XLM-RoBERTa}, aprimoraram o PLN ao oferecer representações contextuais mais detalhadas e precisas. 
No presente estudo, esses modelos foram empregados para sumarização e extração de termos factuais, com destaque para o \textit{SBERT}, 
que apresentou melhor equilíbrio entre desempenho e tempo de processamento.

\begin{figure}[H] % ou [!htb] se preferir deixar o LaTeX decidir
    \centering
    \includegraphics[width=0.7\textwidth]{figs/transformers.png} % coloque o arquivo na pasta escolhida
    \caption{\textit{Transformer - Modelo de Arquitetura}}
    \label{fig:diagrama_tranformer}
    \legend{\footnotesize Fonte: \cite{vaswani2017attention}} % abnTeX2
\end{figure}

A capacidade dos modelos baseados em \textit{Transformers} de compreender o contexto global das sentenças e aprender relações semânticas complexas os torna ferramentas fundamentais para o combate à desinformação, especialmente em ambientes multilíngues, como o português.

Além disso, a formulação original dos \textit{Transformers} parte da ideia de que uma sequência pode ser decomposta em um conjunto de \textit{tokens} sobre os quais se aplica, de forma iterativa, um bloco composto por autoatenção \textit{multi-head} (\textit{MHSA}) e uma rede neural \textit{perceptron} multicamada (\textit{MLP}) posicionada por \textit{token}. Em cada bloco, o mesmo vetor de entrada $x_i$ é projetado em três espaços lineares distintos, gerando $q_i = x_i W_Q$, $k_i = x_i W_K$ e $v_i = x_i W_V$. O parâmetro $q_i$ passa a representar a demanda informacional do \textit{token}, o parâmetro $k_i$ os atributos sob os quais esse \textit{token} pode ser recuperado pelos demais, e o parâmetro $v_i$ o conteúdo semântico efetivamente agregável.

A \textit{MLP} posicionada por \textit{token}, definida por
\[
\mathrm{MLP}(x) = \phi(x W_1 + b_1) W_2 + b_2,
\]
em que $\phi(\cdot)$ denota uma função de ativação não linear (tipicamente 
\textit{ReLU} ou \textit{GELU}). Explicitando essa operação por \textit{token} $i$, temos 
$h_i = \phi(\tilde{x}_i W_1 + b_1)$ e $z_i = h_i W_2 + b_2$, com posterior 
aplicação da conexão residual e da normalização em camadas (\textit{Layer Normalization}), de modo que 
$x_i' = \mathrm{LayerNorm}(\tilde{x}_i + z_i)$~\cite{vaswani2017attention}.

A autoatenção calcula, para cada posição, coeficientes de ponderação assimétricos a partir dos produtos escalares direcionados $q_i k_j^{\top}$, o que permite selecionar, de todo o enunciado, justamente os \textit{tokens} mais relevantes e, assim, modelar dependências de longo alcance sem recorrer à recorrência. Apenas os valores são combinados $z_i = \sum_j \alpha_{ij} v_j$ e o resultado não substitui a representação original: ele é reinjetado por meio de uma conexão residual e estabilizado por \textit{Layer Normalization}, garantindo que cada camada aprenda apenas um refinamento suave da representação anterior e que o empilhamento profundo não degrade o contexto \cite{introTransformers}. 

Essa combinação de atenção global, paralelizável e ligada por conexões residuais explica por que \textit{Transformers} superam \textit{RNNs} e Redes Neurais Convolucionais \textit{CNNs} em tarefas de PLN que exigem contexto amplo e comparações cruzadas entre \textit{tokens}.

\section{Extração de Termos-Chave}

A extração de termos-chave é um processo com potencial significativo de otimizar o pré-processamento textual, cujo objetivo é identificar automaticamente as palavras ou expressões mais representativas de um texto. Essa atividade permite resumir o conteúdo, facilitar a indexação e apoiar processos de classificação e análise semântica.  

As abordagens tradicionais baseiam-se em métodos estatísticos, como \textit{TF-IDF}, \textit{RAKE} (\textit{Rapid Automatic Keyword Extraction}), \textit{TextRank} \cite{mihalcea2004textrank} e \textit{YAKE} (\textit{Yet Another Keyword Extractor}). Tais métodos consideram a frequência e coocorrência de termos, oferecendo soluções rápidas e interpretáveis, embora limitadas no entendimento de contexto.  

Em contrapartida, abordagens modernas utilizam embeddings semânticos e modelos supervisionados para capturar significados contextuais. Técnicas baseadas em \textit{sentence embeddings}, como as fornecidas pelo SBERT~\cite{reimers2019sentencebert}, avaliam a similaridade de cosseno entre frases e palavras, permitindo identificar termos de maior relevância sem depender exclusivamente de frequência estatística. Além disso, modelos supervisionados e híbridos, que combinam embeddings com classificadores como SVMs ou redes neurais, apresentam resultados expressivos em tarefas de extração semântica de palavras-chave~\cite{umair2024plmkeyphrase}.

\section{\textit{Claim Detection} e Verificação de Fatos Automatizada}

O processo de verificação de fatos, ou \textit{fact-checking}, é composto por quatro etapas principais: (1) monitoramento de informações, (2) identificação de afirmações verificáveis (\textit{claim detection}), (3) verificação de veracidade e (4) publicação dos resultados \cite{claimDetection}. Entre essas etapas, a detecção automatizada de afirmações factuais tem ganhado destaque por reduzir o volume de informações analisadas manualmente e otimizar o trabalho das ``agências'' de checagem.  

A detecção de afirmações (\textit{claim detection}) é uma tarefa do PLN voltada à identificação, em um texto, de declarações que expressam uma posição argumentativa, isto é, sentenças que sustentam ou contestam uma determinada ideia. No caso da detecção de afirmações dependentes de contexto (\textit{Context Dependent Claim Detection – CDCD}), proposta por Levy et al. \cite{claimDetectionExplicacao}, o objetivo é reconhecer apenas as afirmações que se relacionam diretamente a um tópico específico, como uma questão controversa ou um tema de debate. Essa abordagem requer não apenas o reconhecimento da estrutura linguística de uma afirmação, mas também a compreensão de sua relevância semântica em relação ao contexto, diferenciando-a de simples definições, repetições do tópico ou informações neutras.

Afirmações detectadas podem ser tanto factuais quanto não factuais, abrangendo desde proposições verificáveis, baseadas em dados concretos, até declarações de natureza opinativa, que refletem juízos de valor, crenças ou percepções subjetivas. Essa característica torna o \textit{claim detection} uma ferramenta importante para tarefas de mineração de argumentação, análise de debates e suporte à decisão automatizada, pois permite extrair, de grandes volumes de texto, as bases argumentativas que sustentam diferentes pontos de vista, independentemente de sua comprovação empírica.

A partir dessa perspectiva argumentativa, a tarefa de \textit{claim detection} passou a ser também entendida como uma etapa inicial fundamental dos sistemas de checagem automática de fatos (\textit{automated fact-checking}). Nesse contexto, o objetivo não é apenas identificar sentenças argumentativas, mas delimitar quais delas são potencialmente verificáveis, isto é, quais expressam afirmações que podem ser submetidas a um processo de comprovação de veracidade com base em evidências empíricas ou fontes confiáveis. Essa evolução conceitual é explorada por Konstantinovskiy et al. \cite{claimDetection}, que desenvolvem um modelo de anotação e um conjunto de dados voltados especificamente à identificação de afirmações factuais.

Os autores propõem uma abordagem mais objetiva e padronizada, evitando critérios subjetivos como “importância” ou “relevância política” da afirmação, frequentemente presentes em trabalhos anteriores. Para isso, elaboraram um esquema de sete categorias de afirmações, abrangendo desde sentenças quantitativas, predições e relações de causa e efeito até leis, regras e experiências pessoais. O modelo classifica como trechos factuais aquelas sentenças que expressam asserções sobre o mundo passíveis de verificação, distinguindo-as de opiniões, perguntas ou comentários.

Assim, o \textit{claim detection} deixa de ser apenas uma tarefa voltada à análise argumentativa e passa a ocupar um papel central na detecção e combate à desinformação. Ao automatizar a identificação de enunciados verificáveis em discursos políticos, notícias ou redes sociais, essa técnica contribui para acelerar o processo de verificação, ampliar a cobertura dos sistemas de \textit{fact-checking} e reduzir o tempo de exposição pública de informações falsas ou enganosas. Desse modo, o campo evolui de uma abordagem semântico-argumentativa para uma função prática e social, a de servir como ponte entre o PLN e a integridade informacional no espaço público.

Baseado na arquitetura \textit{Transformer} de Vaswani et al. \cite{vaswani2017attention}, o \textit{XLM-R} adota um processo de pré-treinamento não supervisionado fundamentado na tarefa de \textit{Masked Language Modeling (MLM)}, em que determinados \textit{tokens} de uma sequência são mascarados e o modelo deve prever as palavras originais com base no contexto \cite{xlm_r}. Essa abordagem permite o aprendizado de representações contextuais profundas, que capturam relações sintáticas e semânticas compartilhadas entre diferentes idiomas. Para alcançar tal desempenho, foram desenvolvidas duas configurações de arquitetura: o \textit{XLM-R Base}, com 12 camadas e cerca de 270 milhões de parâmetros, e o \textit{XLM-R Large}, com 24 camadas e aproximadamente 550 milhões de parâmetros, ambas empregando 16 cabeças de atenção e uma dimensão de \textit{embedding} de 1024 unidades.

O treinamento do \textit{XLM-R} foi realizado sobre o corpus \textit{CC-100}, um conjunto de dados de 2,5 terabytes de textos coletados e filtrados do \textit{CommonCrawl}, abrangendo 100 idiomas. Diferentemente de modelos anteriores, que utilizavam dados da \textit{Wikipédia}, o \textit{XLM-R} amplia significativamente a cobertura linguística, especialmente em línguas de poucos recursos, por meio da filtragem automatizada de textos com o \textit{fastText} e modelos próprios de detecção de idioma. Para tokenização, foi empregado o algoritmo \textit{SentencePiece}, que realiza a segmentação sublexical diretamente sobre texto cru, sem necessidade de regras específicas por idioma, permitindo um vocabulário unificado de 250 mil subpalavras. Esse design elimina dependências linguísticas e torna o modelo mais eficiente em cenários de \textit{code-switching}, onde há mistura de línguas em uma mesma sentença.

Os experimentos conduzidos confirmaram a superioridade do \textit{XLM-R} em relação a modelos como o \textit{mBERT} e o \textit{XLM-100}. Nos testes de inferência multilíngue (\textit{XNLI}), o modelo alcançou ganhos médios de 14,6\% em acurácia, com destaque para melhorias expressivas em idiomas de baixo recurso, como suaíli (+15,7\%) e urdu (+11,4\%). Em tarefas de reconhecimento de entidades nomeadas (\textit{NER}) e resposta a perguntas multilíngue (\textit{MLQA}), o \textit{XLM-R} obteve incrementos de 2,4\% e 13\% em \textit{F1-score}, respectivamente. O modelo também demonstrou desempenho comparável a modelos monolíngues de ponta, como \textit{RoBERTa} e \textit{XLNet}, no \textit{benchmark GLUE}, reforçando a viabilidade de um modelo unificado capaz de lidar com múltiplos idiomas sem perda significativa de desempenho individual.

Assim, o \textit{XLM-R} representa um avanço metodológico e arquitetural significativo para o campo de PLN, provando que a escala e a diversidade linguística, quando combinadas a um treinamento profundo e bem balanceado, são suficientes para gerar representações universais e transferíveis. Essa robustez técnica torna o \textit{XLM-R} uma base sólida para aplicações contemporâneas de detecção de afirmações e checagem automática de fatos, permitindo o \textit{fine-tuning} de modelos especializados, como o \textit{XLM-R-Large-ClaimDetection} utilizado no trabalho, em tarefas que exigem discernimento semântico e generalização entre diferentes idiomas e domínios textuais.

Com base no \textit{XLM-R}, houve uma adaptação supervisionada para a tarefa de classificação de sentenças factuais e não factuais. Esse procedimento tem como objetivo aproveitar o conhecimento linguístico geral aprendido durante o pré-treinamento massivo em múltiplos idiomas e ajustá-lo para um domínio mais restrito, neste caso, o de checagem automatizada de fatos (\textit{automated fact-checking}) \cite{xlm_r_claim_detection}.

Tecnicamente, o \textit{fine-tuning} envolve a reutilização dos pesos e \textit{embeddings} do modelo \textit{XLM-R Large}, previamente treinado com o objetivo de predição de palavras mascaradas (\textit{Masked Language Modeling}), e a adição de uma camada de classificação no topo da rede, responsável por prever se uma sentença representa ou não uma afirmação factual. Durante o treinamento supervisionado, essa nova camada, e, em parte, as camadas internas do \textit{Transformer}, passam por um processo de reajuste dos parâmetros com base em exemplos anotados, permitindo que o modelo aprenda padrões semânticos e discursivos associados a declarações verificáveis.

O processo de adaptação seguiu uma estratégia semi-supervisionada (\textit{weakly-supervised}) em duas etapas. Na primeira, o modelo foi treinado com um conjunto de dados fraco, isto é, um \textit{corpus} de mensagens do \textit{Telegram} anotado automaticamente com o auxílio do modelo \textit{GPT-4o}, que produziu rótulos aproximados de factualidade com base em um \textit{prompt} de classificação. Essa etapa forneceu uma ampla base de exemplos. Em seguida, na segunda etapa, foi realizado um \textit{fine-tuning} mais preciso utilizando o conjunto de dados manualmente anotado proveniente de comentários do \textit{Facebook}. Essa combinação de dados fracos e fortes possibilitou ao modelo consolidar o aprendizado.

O modelo resultante foi então avaliado em dois contextos distintos. Em um conjunto de mensagens do \textit{Telegram} anotadas por quatro codificadores humanos, atingiu uma acurácia de 0,90, demonstrando alta consistência com o julgamento humano. No conjunto de teste com dados extraídos do \textit{Facebook}, obteve-se uma acurácia de 0,79, o que evidencia a capacidade do modelo de se generalizar para diferentes domínios de texto. Embora o treinamento tenha sido realizado exclusivamente com dados em alemão, o modelo mantém o caráter multilíngue herdado do \textit{XLM-R}, que foi originalmente treinado em cem idiomas.

\section{Métricas de desempenho}

\subsection{Acurácia}

A acurácia é uma métrica amplamente utilizada para avaliar o desempenho de modelos de classificação. Ela é definida como a proporção de previsões corretas em relação ao total de previsões realizadas. A fórmula para calcular a acurácia é dada por:
\begin{equation}
    \text{Acurácia} = \frac{TP + TN}{TP + TN + FP + FN}
\end{equation}
onde:
\begin{itemize}
    \item \(TP\) (\textit{True Positives}): número de verdadeiros positivos, ou seja, casos em que o modelo previu corretamente a classe positiva.
    \item \(TN\) (\textit{True Negatives}): número de verdadeiros negativos, ou seja, casos em que o modelo previu corretamente a classe negativa.
    \item \(FP\) (\textit{False Positives}): número de falsos positivos, ou seja, casos em que o modelo previu incorretamente a classe positiva.
    \item \(FN\) (\textit{False Negatives}): número de falsos negativos, ou seja, casos em que o modelo previu incorretamente a classe negativa.
\end{itemize}

\subsection{Precisão}

A precisão é uma métrica que avalia a qualidade das previsões positivas feitas por um modelo de classificação. Ela é definida como a proporção de verdadeiros positivos em relação ao total de previsões positivas. A fórmula para calcular a precisão é dada por:
\begin{equation}
    \text{Precisão} = \frac{TP}{TP + FP}
\end{equation}
onde:
\begin{itemize}
    \item \(TP\) (\textit{True Positives}): número de verdadeiros positivos.
    \item \(FP\) (\textit{False Positives}): número de falsos positivos.
\end{itemize}

\subsection{\textit{Recall}}

O \textit{recall}, ou sensibilidade, é uma métrica que avalia a capacidade de um modelo de classificação em identificar corretamente todas as instâncias positivas. É definido como a proporção de verdadeiros positivos em relação ao total de instâncias positivas reais. A fórmula para calcular o \textit{recall} é dada por:
\begin{equation}
    \text{Recall} = \frac{TP}{TP + FN}
\end{equation}
onde:
\begin{itemize}
    \item \(TP\) (\textit{True Positives}): número de verdadeiros positivos.
    \item \(FN\) (\textit{False Negatives}): número de falsos negativos.
\end{itemize}

\subsection{\textit{F1-Score}}

O \textit{F1-score} é uma métrica que combina a precisão e o \textit{recall} em uma única medida, proporcionando um equilíbrio entre essas duas métricas. Especialmente útil quando há um desequilíbrio entre as classes positivas e negativas. A fórmula para calcular o \textit{F1-score} é dada por:
\begin{equation}
    \text{F1-Score} = 2 \cdot \frac{\text{Precisão} \cdot \text{Recall}}{\text{Precisão} + \text{Recall}}
\end{equation}

\subsection{Índice de Jaccard}

O índice de Jaccard, também conhecido como coeficiente de Jaccard, é uma métrica que avalia a similaridade entre dois conjuntos. É definido como a razão entre a interseção e a união dos conjuntos. A fórmula para calcular o índice de Jaccard é dada por:
\begin{equation}
    \text{Jaccard} = \frac{|A \cap B|}{|A \cup B|}
\end{equation}
onde:
\begin{itemize}
    \item \(A\) e \(B\) são os conjuntos a serem comparados.
\end{itemize}

\section{Trabalhos Relacionados}

A identificação de desinformação e a análise de posicionamentos têm sido amplamente investigadas na literatura devido à sua relevância para a compreensão dos fenômenos sociais, bem como pelo impacto cultural, político e econômico associado a esses temas. Nesta seção, são apresentados os principais trabalhos relacionados a essa temática, com destaque para suas contribuições, metodologias aplicadas e os pontos de melhoria que motivaram e influenciaram o presente estudo.
Inicialmente, discutimos pesquisas com escopo geral voltadas para a classificação de desinformação, cujo aprimoramento é um dos objetivos deste trabalho. Em seguida, apresentamos o estudo inicial que serviu como base para o desenvolvimento tanto do primeiro artigo quanto desta pesquisa, corroborando para a construção de uma base sólida para estes estudos.

O primeiro trabalho em questão, ~\cite{raphaelIC}, tem como foco a análise do posicionamento dos usuários em discussões \textit{online} sobre desinformação. A pesquisa investiga como os posicionamentos expressos em textos podem ser utilizados para identificar conteúdos potencialmente enganosos ou nocivos, com especial atenção às discussões sobre as urnas eletrônicas no Brasil, no período de fevereiro a novembro de 2022. Utilizando técnicas algorítmicas, o estudo aplica modelagem de tópicos e análise de interações nas redes sociais, com dados extraídos de postagens publicadas no \textit{Twitter} (atualmente \textit{X}) e conteúdo desinformativo verificado por ``agências'' de checagem de notícias. Tal extração textual foi auxiliada pelo processo manual na elaboração de termos-chave para obtenção de conteúdo com desinformação, sendo esta a principal lacuna que o presente estudo pretende preencher por meio da elaboração automatizada de termos-chave.

A pesquisa demonstrou a viabilidade da aplicação de técnicas de detecção de posicionamento e modelagem de tópicos para identificar desinformação e caracterizar o comportamento interacional dos usuários na propagação desse conteúdo, utilizando também técnicas de \textit{TF-IDF} para rotulação e análise \cite{raphaelIC}.

O segundo trabalho, ~\cite{automaticDetection}, tem como objetivo analisar os ataques ao sistema eleitoral brasileiro durante as eleições de 2022, especificamente no \textit{Twitter}. Adotando uma abordagem interdisciplinar e o uso de ferramentas computacionais de rotulação automatizada de perfis e análise de linguagem natural, o artigo identificou os principais tipos de discursos hostis e o posicionamento político dos perfis responsáveis por esses tipos de discursos contra as urnas eletrônicas, o Tribunal Superior Eleitoral (TSE) e os magistrados do tribunal. Os dados revelaram que perfis governistas, principalmente bolsonaristas, foram os responsáveis por uma maior produção de conteúdos hostis, incluindo xingamentos às urnas eletrônicas e ataques de ódio direcionados aos ministros do TSE.

Outro projeto relevante na detecção de textos factuais utilizado como base para este trabalho é o estudo de Arslan et al. \cite{dbClaim}, que introduz o conjunto de dados \textit{ClaimBuster}, um \textit{benchmark} para identificação de afirmações passíveis de checagem (\textit{check-worthy factual claims}) em discursos políticos. O corpus é composto por 23,533 sentenças extraídas das transcrições de todos os debates presidenciais gerais dos Estados Unidos, anotadas manualmente em três categorias: (i) enunciados não factuais, (ii) enunciados factuais pouco relevantes e (iii) enunciados factuais relevantes para checagem. Essa estrutura de rótulos permite distinguir, dentro do subconjunto de sentenças factuais, aquelas que efetivamente merecem prioridade em processos de checagem profissional.

Os autores formalizam afirmações \textit{check-worthy} como declarações factuais cujo esclarecimento de veracidade é, em princípio, de interesse do público em geral, e documentam cuidadosamente o processo de anotação, incluindo diretrizes e agregação das decisões individuais dos anotadores \cite{dbClaim}. O conjunto é disponibilizado em diferentes arquivos, contendo tanto as sentenças quanto os rótulos consolidados, o que viabiliza estudos sobre subjetividade na tarefa e sobre a consistência entre avaliadores humanos.

A partir desse recurso, Arslan et al. \cite{dbClaim} avaliam modelos supervisionados para detecção automática de afirmações passíveis de checagem, combinando características lexicais e semânticas das sentenças com algoritmos de aprendizado de máquina. Os resultados mostram que é possível discriminar, com desempenho competitivo, entre sentenças não factuais, factuais pouco relevantes e factuais relevantes, estabelecendo um patamar de comparação para trabalhos posteriores em \textit{claim detection}. Assim, a principal contribuição do artigo é fornecer um conjunto de dados padronizado e amplamente reutilizável que serve como base para o desenvolvimento e a avaliação sistemática de modelos de identificação de afirmações factuais verificáveis em textos políticos.

Ademais, o estudo aplicou um método de detecção de posicionamento não supervisionado para agrupar os usuários em \textit{clusters} polarizados, com base nas contas que retuitaram. Essa técnica foi fundamentada em estudos que sugerem que os usuários tendem a polarizar suas opiniões e formar comunidades políticas, seguindo o princípio da homofilia. A análise revelou dois grandes \textit{clusters}: um representando usuários favoráveis à visão política dominante e outro, contrário. Esses \textit{clusters} mostraram-se densos em interações internas, com baixa proximidade entre si, caracterizando a polarização do debate. A abordagem permitiu entender melhor as dinâmicas de propagação de discursos tóxicos e a formação de ``bolhas'' ideológicas dentro da rede social, reforçando a relevância da polarização como um fator na disseminação de desinformação \cite{automaticDetection}.
Esses \textit{clusters} polarizados também foram utilizados para testar a extração de tuítes com base nos termos-chave obtidos nesta pesquisa, assim como foi feito com os termos-chave extraídos manualmente na primeira pesquisa citada. Portanto, esse artigo foi essencial para a comparação entre os métodos automáticos e manuais de extração de termos-chave que permitiu avaliar a eficácia da abordagem automatizada, oferecendo informações fundamentais sobre a viabilidade de sua aplicação em contextos de grande volume de dados.
