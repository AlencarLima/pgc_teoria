\chapter{Resultados e Discussões}
\label{cap:resultados}
\section{Resultados Obtidos}

Este capítulo apresenta e discute os principais resultados empíricos obtidos ao longo do estudo. Inicialmente, são analisados os efeitos dos diferentes modelos de sumarização automática na redução de ruído e na preservação de trechos factuais dos textos. Em seguida, são avaliadas as abordagens de segmentação baseadas em conjunções e em aspas, com ênfase na capacidade de recuperar trechos relevantes em notícias e em \textit{clusters} de \textit{tweets}. Na sequência, discute-se o desempenho do modelo \textit{XLM-R-Large-ClaimDetection} e do classificador SVM na identificação de afirmações factuais, comparando seus resultados com o modelo original em inglês e com o método manual de referência. Por fim, são comparadas as diferentes estratégias de classificação de desinformação em \textit{tweets} e retuítes, destacando os compromissos entre assertividade, conservadorismo e risco de super-rotulagem.


\subsection{Sumarização}
Para o primeiro processo de sumarização automática para redução de ruído, três modelos foram avaliados: BERT-base, DistilBERT e SBERT-MiniLM. Em todos os casos, os resumos ficaram entre 25\% e 27\% do tamanho do texto original conforme mostrado na Figura~\ref{fig:compression-ratio}, sem diferenças expressivas de compressão entre os modelos, com leve tendência do DistilBERT a produzir resumos mais concisos e do SBERT-MiniLM a manter textos um pouco maiores.

\begin{figure}[ht] % ou [!htb] se preferir deixar o LaTeX decidir
    \centering
    \includegraphics[width=1\textwidth]{figs/compression_ratio.png} % coloque o arquivo na pasta escolhida
    \caption{\textit{Compression ratio} médio (|resumo|/|original|) por modelo.}
    \label{fig:compression-ratio}
    \legend{\footnotesize Fonte: elaboração própria.} % abnTeX2
\end{figure}

Quando se observa apenas a preservação de trechos factuais, BERT-base e SBERT-MiniLM mantiveram cerca de 25\% a 26\% de conteúdo classificado como factual, superando o DistilBERT, que ficou em torno de 22\% conforme mostrado na Figura~\ref{fig:factualidade}. Em termos de retenção de tópicos centrais, medida pelo índice de Jaccard entre as palavras-chave do original e do resumo, o SBERT-MiniLM apresentou o melhor resultado (0,22), seguido muito de perto pelo BERT-base (0,21), enquanto o DistilBERT apresentou desempenho inferior (0,14). Da mesma forma, quando a similaridade textual foi medida por uma métrica de similaridade (TF-IDF/cosseno), o SBERT-MiniLM obteve o maior valor (0,64), superando ligeiramente o BERT-base e o DistilBERT, ambos com 0,61. No entanto, o tempo de processamento mostrou diferenças marcantes: o BERT-base demandou em média mais de 22 segundos por texto, o DistilBERT cerca de 3 segundos e o SBERT-MiniLM pouco mais de 1 segundo, o que torna o último mais adequado a cenários de grande volume.

\begin{figure}[ht] % ou [!htb] se preferir deixar o LaTeX decidir
    \centering
    \includegraphics[width=1\textwidth]{figs/factual_graph.png} % coloque o arquivo na pasta escolhida
    \caption{Factualidade \% por modelo.}
    \label{fig:factualidade}
    \legend{\footnotesize Fonte: elaboração própria.} % abnTeX2
\end{figure}

\subsection{Segmentação baseada em conjunções}
Os resultados da abordagem de segmentação baseada no termo ``que'' demonstraram sua eficácia. Nas 29 amostras de notícias provenientes de ``agências'' de checagem de fatos, a segmentação baseada no termo ``que'' resultou em textos altamente relevantes, evidenciando ser uma técnica interessante de segmentação para textos contidos em notícias. Este método apresenta a vantagem de não exigir grande processamento, sendo uma alternativa eficiente para identificar informações relevantes em grandes volumes de texto.

Além disso, a aplicação desse método em \textit{clusters} polarizados mostrou resultados significativos. Para o $cluster_0$, foi possível obter 2,060 \textit{tuítes} relevantes, em comparação com os 2,056 \textit{tuítes} obtidos utilizando termos-chave criados manualmente. Já no $cluster_1$, foram extraídos 1,089 \textit{tuítes}, superando os 781 obtidos com termos-chave definidos manualmente. Exemplos de textos retornados por esse método incluem: ``aponta que apertar `confirma' durante a tela `confira seu voto' anula o voto'' e ``usou o plenário da câmara federal para propagar informações falsas sobre a pandemia, como mostrou uma reportagem publicada pela Lupa em dezembro do ano passado''. Esses resultados demonstram que a segmentação baseada em conjunções, como o termo ``que'', pode ser mais eficaz do que abordagens manuais tradicionais, ampliando a abrangência e precisão na identificação de conteúdos relevantes para análise de desinformação.

A aplicação do modelo XLM-R-Large-\textit{ClaimDetection} nos dados de debates presidenciais dos EUA ~\cite{dbClaim}, adaptados ao contexto português, apresentou resultados promissores na tarefa de classificação de afirmações factuais. O modelo alcançou uma acurácia de 0,88, mostrando bom desempenho mesmo considerando as diferenças linguísticas entre os dados testados em português e o treinamento original em inglês.

Por meio da utilização de uma matriz de confusão foi possível verificar que o modelo classificou corretamente a maioria das afirmações factuais e não factuais, com 680 previsões corretas para sentenças factuais não relevantes e 232 para as sentenças factuais importantes. Porém, houve dificuldades de diferenciar categorias factuais não relevantes, sendo classificadas incorretamente 114 sentenças das 1034 presentes no \textit{database}.

Esses resultados são demonstrados no relatório de classificação, que apresentou um desempenho muito elevado para as sentenças factuais não relevantes, com \textit{precisão} de 0,99 e \textit{recall} de 0,86, conforme dados \autoref{tab:resultados_pt}. Já para as sentenças factuais relevantes, a \textit{precisão} foi de 0,67, enquanto o \textit{recall} atingiu um valor consideravelmente alto de 0,97, evidenciando a capacidade do modelo em identificar corretamente a maioria das afirmações importantes, ainda que tenha apresentado menor precisão nessa categoria.

Em comparação com o modelo original treinado em inglês, que obteve uma acurácia de 0,90, os resultados em português apresentaram uma leve queda. No estudo original, o modelo foi capaz de classificar afirmações factuais e não factuais com equilíbrio entre \textit{precisão} e \textit{recall}, alcançando valores de 0,90 em ambas as métricas, conforme apresentado na \autoref{tab:resultados_en}.

\begin{table}[ht]
    \centering
    \caption{Resultados do modelo XLM-R-Large-\textit{ClaimDetection} em português}
    \label{tab:resultados_pt}
    \begin{tabular}{lccc}
        \toprule
        Classe & \textit{Precision} & \textit{Recall} & \textit{F1-Score} \\
        \midrule
        \textit{Unimportant Factual Sentence (UFS)} & 0,99 & 0,86 & 0,92 \\
        \textit{Important Factual Sentence (IFS)}   & 0,67 & 0,97 & 0,79 \\
        \midrule
        Acurácia Geral & \multicolumn{3}{c}{0,88} \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[ht]
    \centering
    \caption{Resultados originais do modelo XLM-R-Large-\textit{ClaimDetection} em inglês}
    \label{tab:resultados_en}
    \begin{tabular}{lccc}
        \toprule
        Classe       & \textit{Precision} & \textit{Recall} & \textit{F1-Score} \\
        \midrule
        \textit{Factual}      & 0,88      & 0,92   & 0,90     \\
        \textit{Non-Factual}  & 0,92      & 0,88   & 0,90     \\
        \midrule
        Acurácia Geral & \multicolumn{3}{c}{0,90} \\
        \bottomrule
    \end{tabular}
\end{table}

O baixo rendimento em encontrar sentenças factuais importantes (IFS) pode ser atribuído ao fato de o treinamento não ter sido realizado de forma ideal, devido a limitações de \textit{hardware}. O desempenho alcançada, ainda assim, destaca a utilidade do modelo XLM-R-Large como ferramenta para filtragem e segmentação de grandes volumes de texto, facilitando a identificação de informações relevantes para análises de desinformação e processos de verificação de fatos.

Apesar de apresentar métricas menos eficientes, o modelo conseguiu extrair textos relevantes das 29 notícias analisadas conforme pode ser visto na \autoref{tab:exemplo-6x5}. Com textos resumidos pelo BERT, foram retornados 2,391 \textit{tuítes} no $cluster_0$ e 1,333 no $cluster_1$, já para o DistilBERT (dBERT), 1,670 e 928, respectivamente e com o SBERT, 2,040 e 1,234. Entre as frases relevantes extraídas, destacam-se exemplos como: “o TSE (Tribunal Superior Eleitoral) não testou a segurança das urnas para as eleições de 2022”. No entanto, o modelo também retornou frases menos relevantes, como: “no Telegram disseminam informações falsas sobre o”, “estão registrados na seção” e “circulam nas redes sociais (veja aqui)”. Esse comportamento reforça a necessidade de aprimorar a precisão na filtragem de conteúdo, evitando a extração de dados não essenciais, o que pode aumentar o tempo de processamento e reduzir a eficiência geral do sistema.

\subsection{Segmentação baseada em aspas}
Para o método envolvendo apenas aspas, houve retorno de frases para apenas 5 das 29
notícias analisadas, demonstrando sua baixa eficácia na identificação de informações relevantes. Entre os textos retornados estavam exemplos como: ``Outro exemplo citado por ele é no caso de a totalização envolver, hipoteticamente, apenas duas cidades'', além de termos como ``banco nacional do brasil'', ``sala escura'' e ``vão para as nuvens''. Embora algumas frases tenham relevância contextual, o método não foi consistente o suficiente para ser amplamente aplicado, destacando a necessidade de estratégias mais eficazes para análise desse tipo de dado.

\subsection{Classificação de afirmações factuais com XLM-R-Large-\textit{ClaimDetection}}
A aplicação do modelo XLM-R-Large-\textit{ClaimDetection} apresentou acurácia de 0,88 em português, com bom \textit{recall} para sentenças factuais importantes (0,97), mas menor precisão (0,67), o que indica tendência a recuperar quase tudo o que é relevante, ao custo de trazer alguns itens não essenciais.

\subsection{Comparação com o SVM}
O modelo SVM apresenta uma acurácia global moderada, em torno de 0,79. A classe 0 obteve \textit{precision} de 0,85 e \textit{recall} de 0,83, enquanto a classe 1, associada às sentenças factuais/relevantes, alcançou \textit{precision} de 0,69 e \textit{f1-score} de 0,71, conforme \autoref{tab:resultados_svm}. Esses valores indicam que o modelo é capaz de capturar uma parcela significativa das sentenças de interesse, embora ainda com ocorrência de falsos positivos e perda de algumas sentenças relevantes. Na prática, isso significa que a filtragem contribui para reduzir o volume de texto a ser analisado. Contudo, a presença de textos menos relevantes entre os resultados sugere que o modelo SVM pode beneficiar-se de ajustes adicionais ou da combinação com outras técnicas de filtragem para melhorar a qualidade do conteúdo extraído.

\begin{table}[ht]
    \centering
    \caption{Resultados do SVM em português}
    \label{tab:resultados_svm}
    \begin{tabular}{lccc}
        \toprule
        Classe        & \textit{Precision} & \textit{Recall} & \textit{F1-Score} \\
        \midrule
        \textit{Factual}      & 0,6903    & 0,7323 & 0,7107 \\
        \textit{Non-Factual}  & 0,8538    & 0,8263 & 0,8398 \\
        \midrule
        Acurácia geral & \multicolumn{3}{c}{0,7938} \\
        \bottomrule
    \end{tabular}
\end{table}

\section{Comparação de Classificação}
Pode-se notar que pelo gráfico de Distribuição de Desinformação em \textit{tweets} presentes no \textit{Cluster} 0 (Figura~\ref{fig:tweets_cluster_0}), no método manual de referência o C0 apresenta 33{,}4\% de \textit{tweets} classificados como com desinformação, 6{,}0\% como sem desinformação e 60{,}5\% como não rotulados. Com a aplicação dos modelos, todos os métodos de segmentação e classificação aumentaram a proporção de \textit{tweets} classificados como com desinformação em relação ao método manual: no DistilBERT, essa proporção sobe para 47{,}5\%, enquanto SBERT, segmentação por ``que'', BERT e SVM deslocam a classe ``com desinformação'' para a faixa de 55--62\%, com a classe ``sem desinformação'' avançando para cerca de 9--13\%. O DistilBERT foi o que apresentou o menor aumento, seguido pelo SBERT, segmentação por ``que'', BERT e SVM, que tiveram aumentos mais expressivos. Isso indica que esses métodos são mais agressivos na identificação de \textit{tweets} com desinformação, o que pode ser benéfico para capturar mais conteúdo relevante, mas também pode aumentar o risco de falsos positivos.

Já para o \textit{Cluster} 1 (Figura~\ref{fig:tweets_cluster_1}), no método manual de referência 8{,}4\% dos \textit{tweets} são classificados como com desinformação, 28{,}6\% como sem desinformação e 63{,}0\% permanecem não rotulados''. Com os modelos, todos os métodos também aumentaram a proporção de \textit{tweets} classificados como sem desinformação em comparação com o método manual, elevando essa classe para a faixa de 42--54\% e a classe com desinformação para 15--17\%, ao mesmo tempo em que comprimem a fração não rotulado para aproximadamente 29--43\%, abaixo dos 63{,}0\% do cenário manual. Novamente, o DistilBERT apresentou o menor aumento, seguido pela segmentação por ``que'', SBERT, BERT e SVM. Isso sugere que esses métodos são eficazes na identificação de \textit{tweets} sem desinformação.

Em termos de comparação relativa ao manual, o DistilBERT tende a ficar mais próximo, sendo o que menos retira a classe ``não rotulado'' (C0: 43{,}3\% e C1: 43{,}2\%). BERT, sBERT e ``que'' tornam as classes rotuladas mais assertivas, principalmente \textit{com desinformação} no C0 e \textit{sem desinformação} no C1. Entre os resultados, o DistilBERT apresenta o desvio mais contido, enquanto BERT/sBERT/``que'' oferecem maior assertividade com menor conservadorismo.

Os gráficos de retuítes, exibidos nas Figuras~\ref{fig:retuites_cluster_0} e \ref{fig:retuites_cluster_1}, mostram padrões semelhantes aos observados nos gráficos de \textit{tweets}. No método manual de referência, o \textit{Cluster} 0 (C0) apresenta 44{,}0\% de retuítes classificados como ``com desinformação'', 11{,}2\% como ``sem desinformação'' e 44{,}9\% como ``não rotulados''. Com a aplicação dos modelos, todos os métodos de segmentação e classificação aumentaram a proporção de retuítes classificados como ``com desinformação'' em relação ao método manual: no DistilBERT, essa proporção sobe para 57{,}0\%, para SBERT, segmentação por ``que'', BERT e SVM deslocam a classe ``com desinformação'' para a faixa de 57--63\%, com a classe ``sem desinformação'' avançando para cerca de 5--13\%. O DistilBERT foi o único que apresentou um decréscimo na classe ``sem desinformação'', onde houve apenas 2154 retuítes para ``sem desinformação'' em comparação aos 4493 no método manual. Os outros métodos aumentaram essa classe em relação ao manual mas de forma pouco significativa.

Já para o \textit{Cluster} 1 (Figura~\ref{fig:retuites_cluster_1}), no método manual de referência 1{,}3\% dos retuítes são classificados como ``com desinformação'', 44{,}5\% como ``sem desinformação'' e 54{,}3\% permanecem ``não rotulados''. Com os modelos, todos os métodos também aumentaram a proporção de retuítes classificados como ``sem desinformação'' em comparação com o método manual, elevando essa classe para a faixa de 51--72\%, ao mesmo tempo em que comprimem a fração ``não rotulado'' para aproximadamente 24--54\%, abaixo dos 62{,}6\% do cenário manual. Novamente, o DistilBERT apresentou o menor aumento, seguido pela segmentação por ``que'', SBERT, BERT e SVM. Já a classe ``com desinformação'' apresentou um aumento porcentual menos significativo em relação ao manual, ficando na faixa de 1{,}6--2{,}3\%, tendo o DistilBERT e SBERT bem próximos ao obtido manualmente (1{,}6 e 1{,}7\% respectivamente).

O DistilBERT atua como um freio nessa tendência. Ele preserva uma parcela maior de ``não rotulado'' em ambos os \textit{clusters}, aproximando a distribuição dos retuítes do que se vê no manual. Em C0, isso se traduz em um crescimento mais moderado da classe ``com desinformação''. Em C1, significa manter, em nível relativamente alto, a incerteza capturada pela classe ``não rotulado'', e, portanto, menor risco de super-rotulagem. Em resumo, entre os modelos testados, DistilBERT apresenta o desvio mais contido em relação ao padrão manual, enquanto BERT, sBERT e ``que'' fornecem leituras mais assertivas.

Do ponto de vista analítico, a opção entre assertividade e fidelidade orienta a escolha. Para reproduzir com maior proximidade o retrato
manual e controlar falsos positivos nos retuítes, DistilBERT é mais similar e retorna resultados mais conservadores. Para maximizar a rotulagem e obter uma visão mais clara dos polos de desinformação e informação correta, BERT, sBERT e a segmentação por ``que'' são mais eficazes.

\begin{figure}[H] % ou [!htb] se preferir deixar o LaTeX decidir
    \centering
    \includegraphics[width=1\textwidth]{figs/cluster_0_tweets.png} % coloque o arquivo na pasta escolhida
    \caption{Distribuição de Desinformação em \textit{tweets} por abordagem para o \textit{Cluster} 0 - Mês de outubro/2022}
    \label{fig:tweets_cluster_0}
    \legend{\footnotesize Fonte: elaboração própria.} % abnTeX2
\end{figure}
\begin{figure}[H] % ou [!htb] se preferir deixar o LaTeX decidir
    \centering
    \includegraphics[width=1\textwidth]{figs/cluster_1_tweets.png} % coloque o arquivo na pasta escolhida
    \caption{Distribuição de Desinformação em \textit{tweets} por abordagem para o \textit{Cluster} 1 - Mês de outubro/2022}
    \label{fig:tweets_cluster_1}
    \legend{\footnotesize Fonte: elaboração própria.} % abnTeX2
\end{figure}
\begin{figure}[H] % ou [!htb] se preferir deixar o LaTeX decidir
    \centering
    \includegraphics[width=1\textwidth]{figs/cluster_0_retuites.png} % coloque o arquivo na pasta escolhida
    \caption{Distribuição de Desinformação em retuítes por abordagem para o \textit{Cluster} 0 - Mês de outubro/2022}
    \label{fig:retuites_cluster_0}
    \legend{\footnotesize Fonte: elaboração própria.} % abnTeX2
\end{figure}
\begin{figure}[H] % ou [!htb] se preferir deixar o LaTeX decidir
    \centering
    \includegraphics[width=1\textwidth]{figs/cluster_1_retuites.png} % coloque o arquivo na pasta escolhida
    \caption{Distribuição de Desinformação em retuítes por abordagem para o \textit{Cluster} 1 - Mês de outubro/2022}
    \label{fig:retuites_cluster_1}
    \legend{\footnotesize Fonte: elaboração própria.} % abnTeX2
\end{figure}

\begin{table}[H]
\centering
\caption{Termos-chave resultantes}
\label{tab:exemplo-6x5}
\renewcommand{\arraystretch}{0,9} % um pouco mais de altura nas linhas
\begin{tabularx}{\linewidth}{|>{\raggedright\arraybackslash}p{0,15\linewidth}|X|X|X|X|}
\hline
\textbf{Títulos das notícias} & \textbf{Termos Manuais} & \textbf{Termos SVM} & \textbf{Termos DistilBERT} & \textbf{Termos SBERT} \\
\hline
Vulnerabilida-des em urnas citadas em vídeo de 2014 já foram corrigidas & fraudar urnas, vídeo de Diego
Aranha, não testou as urnas, como
fiscalizar as urnas, Video viralizando
Sábado, não te como TSE Fraudar as
urnas & os problemas citados por ele na gravação já foram sanados pela justiça eleitoral, os problemas citados por ele na gravação já foram sanados pela justiça eleitoral, um vídeo de 2014 em & registra aleatoriamente os votos computados pelos eleitores, o tse (tribunal superior eleitoral) não testou a segurança das urnas para as eleições de 2022 & o tse (tribunal superior eleitoral) não testou a segurança das urnas para as eleições de 2022 \\
\hline
Apertar “confirma” durante tela “confira seu voto” na urna eletrônica anula voto é boato & TSE fraudar as urnas, fraudar
eleições, derrota antecipada,
fracasso eleitoral, Não votar nas
urnas eletrônicas, muito mais votos
registrados & estamos chegando na semana final da campanha eleitoral do primeiro turno, ao contrário, foi possível ver & apertar “confirma” durante a tela “confira seu voto” anula o voto, anulação é falsa & a tela em questão não tem qualquer relação com anulação \\
\hline
Correntes no WhatsApp e no Telegram mentem sobre o que eleitor pode ou não fazer no domingo & Apertar confira seu voto, perderá
voto, votos não computadorizados,
votos serão anulados, teclar ok ou
confirma & mesários mal-intencionados tentariam boicotar o pleito, deixando de entregar comprovantes de votação, de coletar as assinaturas no livro de registro & no telegram disseminam informações falsas sobre o & aos fatosf: compartilhe correntes de mensagens no whatsapp \\
\hline
Acusação de fraude eleitoral domina correntes de WhatsApp em grupos monitorados & FRAUD3 NA ELE1ÇÃ0,Bolso-naro
não pode deixar haver, Bolsonaro
não deixa ter segundo turno, Fraude
na eleição & esta reportagem foi feita numa colaboração entre agência pública, aos fatos & esta reportagem foi feita numa colaboração entre agência pública, aos fatos & esta reportagem foi feita numa colaboração entre agência pública, aos fatos \\
\hline
\end{tabularx}
\end{table}
