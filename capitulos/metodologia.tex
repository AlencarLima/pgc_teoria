\chapter{Metodologia}
\label{cap:metodologia}
\section{Etapas a Serem Realizadas Neste Projeto}

\subsection{Extração e seleção de produções textuais}
A primeira etapa consistiu na extração de dados para a construção de uma base de conhecimento que foi usada para análise e classificação de novos textos. Para a extração de conteúdos desinformacionais, foi utilizado web scraping (raspagem de dados) com a biblioteca Python BeautifulSoup, que manipula dados HTML e XML de sites. Com essas fontes, foram coletados dados usados como base para obtenção de termos-chave relevantes. Mais especificamente, o web scraping foi realizado em notícias previamente selecionadas por outro estudo ~\cite{raphaelIC} com foco mais geral que utiliza detecção de posicionamento para identificação de conteúdo desinformativo no mesmo contexto das eleições brasileiras de 2022. 

Tais conteúdos relacionados à desinformação foram coletados a partir de postagens previamente verificadas por agências de fact-checking, como G1 Fato ou Fake, Aos Fatos, UOL Confere, Estadão Verifica e Agência Lupa. O processo envolveu uma seleção criteriosa de postagens que continham informações falsas ou enganosas, já analisadas e desmentidas por essas agências.

Além disso, a coleta foi limitada a conteúdos verificados e publicados no período de janeiro a dezembro de 2022, incluindo apenas postagens que continham os termos específicos: `urna', `urnas', `eleição', `voto impresso', `auditável' e `eleições'. Essas restrições foram estabelecidas para garantir que os dados retornados estivessem diretamente relacionados ao contexto das eleições brasileiras de 2022, permitindo uma análise mais focada do conteúdo desinformativo vinculado a esse período crucial. ~\cite{raphaelIC}

Para futura comparação e classificação textual, foram utilizados dados coletados por meio da API v2 do Twitter³. A API (Interface de Programação de Aplicações) é um conjunto de ferramentas que permite que desenvolvedores interajam diretamente com os serviços e dados de uma plataforma, como o Twitter. Por meio dela, é possível acessar informações estruturadas, como tweets, retweets, curtidas, hashtags e metadados, de forma programática e escalável.

Entretanto, com a transição da API do Twitter para um modelo monetizado com altos custos, a extração contínua de conteúdo tornou-se inviável. Ainda assim, os dados obtidos anteriormente ~\cite{automaticDetection} foram suficientes para sustentar pesquisas relacionadas, incluindo o presente estudo.

\subsection{Pré-processamento e mapeamento de textos}
Os textos extraídos devem passar por diversas etapas de normalização textual, com o objetivo de padronizar os textos para uma forma mais conveniente para serem processados por
meio das tecnologias utilizadas para classificação textual. Tokenização, processo também
conhecido como segmentação de palavras, quebra a sequência de caracteres localizando o
limite de cada palavra, ou seja, diferencia onde uma palavra inicia e outra começa
~\cite{barbosa2017}. Remoção de stopwords, também chamadas de palavras redundantes, o
que possibilita retirar palavras que não acrescentam ou não interferem no significado do texto analisado de forma relevante. Lematização, processo para reduzir palavras através do infinitivo dos verbos e masculino singular dos substantivos e adjetivos ~\cite{lemaVSStemming}. Identificação de palavras-chave, técnica que possibilita a extração de palavras mais relevantes para o significado do conteúdo, o que além de facilitar a identificação de entidades e relações presentes no texto, permite uma redução da base de dados a serem analisadas.

O primeiro passo no processo foi a extração de sentenças relevantes por meio do cálculo do peso TF-IDF (Term Frequency-Inverse Document Frequency), uma métrica amplamente utilizada para avaliar a relevância de palavras e sentenças dentro de um conjunto de textos. A função desenvolvida para este fim utiliza a classe TfidfVectorizer da biblioteca Scikit-learn, que converte sentenças de uma notícia em vetores numéricos representando a importância de termos no contexto do documento analisado. Adicionalmente, a similaridade de cosseno entre as sentenças também foi calculada, permitindo identificar trechos que melhor resumem o conteúdo do texto, com base em sua relevância estatística.

Os dados coletados em fontes de agências de fact-checking foram processados utilizando técnicas de sumarização automatizada, especificamente por meio do modelo BERT (Bidirectional Encoder Representations from Transformers), com o objetivo de gerar resumos mais concisos e focados. O BERT é uma tecnologia baseada na arquitetura de Transformers, que utiliza atenção bidirecional para compreender o contexto das palavras em um texto. Essa abordagem permite capturar características semânticas tanto de palavras individuais quanto de frases inteiras, resultando em resumos que preservam o significado essencial do conteúdo original.

O processo de sumarização foi realizado utilizando uma variante específica do BERT, chamada SBERT (Sentence-BERT), otimizada para tarefas de similaridade e classificação de sentenças. Esse modelo foi treinado para identificar as partes mais relevantes do texto, reduzindo-o a uma forma condensada sem perder informações críticas, facilitando assim a análise de grandes volumes de dados desinformativos. Diversos modelos baseados em Transformers foram testados para identificar a abordagem mais eficiente em termos de desempenho e precisão. Entre os modelos avaliados estavam:
\begin{itemize}
    \item BERT padrão (model = Summarizer()), que apresentou um tempo médio de processamento de 301,84 a 309,03 segundos por tarefa.
    \item BERT com suporte a resolução de correferências, utilizando o CoreferenceHandler com um grau de ``ganância'' (greedyness) ajustado para 0,4.
    \item DistilBERT, uma versão mais leve do BERT, configurada com camadas ocultas específicas (hidden=[-1, -2] e hidden\_concat=True), que reduziu o tempo médio de execução para aproximadamente 86,87 a 96,21 segundos por tarefa.
    \item SBERT (Sentence-BERT), utilizando a versão paraphrase-MiniLM-L6-v2, que apresentou o melhor desempenho, com um tempo médio de processamento de 57,75 a 60,28 segundos por tarefa.
\end{itemize}

O fator determinante para a escolha do SBERT foi, principalmente, o tempo de processamento reduzido, pois os resultados dos resumos gerados não apresentaram diferenças significativas em relação aos outros modelos testados. Além disso, o SBERT demonstrou desempenho consistente tanto na extração de palavras-chave quanto em processos futuros do estudo, como a análise de conteúdo e a comparação de dados.

Combinando a sumarização e a extração de palavras-chave, foi possível obter uma visualização mais clara e organizada dos dados, reduzindo ruídos e destacando elementos fundamentais para a análise do conteúdo desinformativo, especialmente no contexto das eleições brasileiras de 2022.

Os dados para comparação foram previamente processados e analisados, abrangendo o período de julho a novembro de 2022, com foco nos debates políticos no Brasil que antecederam as eleições gerais daquele ano ~\cite{automaticDetection}. Vale destacar que, assim como nos artigos em que este estudo foi baseado, os termos tweets e retweets foram mantidos e não substituídos por ``post'' e ``repost''. Essa decisão segue a terminologia amplamente conhecida e utilizada, em vez de adotar a nova nomenclatura estabelecida pela empresa atualmente denominada X.

\subsection{Registro dos Dados Processados}

O registro dos dados previamente processados foi armazenado em arquivos de documentos, devido ao volume relativamente pequeno de informações. Os textos extraídos por meio do processo de web scraping, bem como as notícias coletadas de agências de fact-checking, foram manipulados utilizando a biblioteca pandas do Python e organizados em arquivos XLSX com tamanhos variando entre 100 KB e 200 KB cada.

Já os dados utilizados para comparações, por apresentarem maior volume, foram armazenados em arquivos CSV, com tamanhos variando entre 5500 KB e 11000 KB, dependendo do conteúdo. Esses dados incluem textos previamente processados e preparados para análise de similaridade e padrões, permitindo uma estrutura eficiente para futuras análises.

\subsection{Obtenção dos Termos-chave}
Para os modelos de classificação semântica e detecção de fatos, foi implementado um processo de segmentação adicional com expressões regulares. Essa etapa visou dividir as sentenças em fragmentos menores com base em conjunções e conectores linguísticos. Além disso, foi aplicada uma estratégia de separação específica para frases resumidas, utilizando o método de divisão split contido na biblioteca regex por conjunções. Essa abordagem permite que cada ideia expressa seja analisada isoladamente, garantindo maior precisão na classificação semântica.

As conjunções são palavras ou expressões que conectam orações ou termos dentro de uma frase, indicando relações como adição, contraste, causa, condição ou tempo. Exemplo: palavras como `e', `mas', `porque' e `enquanto' são conjunções amplamente utilizadas na língua portuguesa. No contexto do processamento de textos, essas palavras são importantes pois frequentemente sinalizam mudanças de foco ou introduzem novas ideias. Dessa forma, pode-se obter a separação dos dados factuais de não factuais necessários, filtrando dados possíveis de se conter desinformação.

Além disso, foi notado que o termo ``que'', em notícias provenientes de agências de checagem de fatos, frequentemente é sucedido por informações relevantes, sejam elas desinformação ou esclarecimentos. Com base nessa observação, o termo foi utilizado como um ponto de referência para identificar e isolar termos-chave no texto, ajudando a localizar trechos particularmente importantes para a análise e classificação semântica.

Outro processamento focado em expressões regulares foi a detecção de aspas (`` ou `) em notícias, uma vez que essas normalmente remetem a citações ou falas que podem indicar pronunciamentos relacionados à desinformação ou apenas relatar declarações factuais.

O aspecto de normalização dos textos, que incluiu a remoção de espaços extras e a conversão de palavras para um formato consistente, também foi considerado, evitando problemas como redundâncias causadas por variações de capitalização, espaçamento ou outras formatações inadequadas capazes de interferir no processo de extração textual. O processo de padding, utilizado para adequar os textos a um tamanho padrão de sequência, desempenha um papel crucial na otimização do processamento em modelos baseados em deep learning. Essa técnica evita problemas de incompatibilidade de tamanho entre lotes e pode reduzir drasticamente a quantidade de tokens desnecessários (padding tokens), que em casos extremos podem representar até 50 por cento dos tokens processados, como relatado em estudos recentes ~\cite{padding}.

Se tratando de abordagens mais complexas, o modelo XLM-R-Large-ClaimDetection foi utilizado para filtrar textos previamente processados, retirados de fontes de checagem de fatos confiáveis, com o objetivo de classificar esses textos como ``factual'' ou ``não factual''. A tarefa principal do modelo foi identificar os textos que possuem afirmações factuais, eliminando aqueles que não necessitam de verificação. É válido ressaltar que inicialmente o modelo foi treinado para dados ``factuais'', ``não factuais'' e ``insignificantes'' e para este projeto apenas dados ``factuais'' foram filtrados. ~\cite{overviewClaim}

O conjunto de dados utilizado para treinamento do modelo ClaimDetection contém 23.533 declarações extraídas de todos os debates presidenciais dos EUA ~\cite{dbClaim}. Essas declarações foram rotuladas por um processo manual e classificadas em três categorias: afirmações factuais dignas de verificação, afirmações factuais insignificantes e afirmações não factuais. Durante o estudo, foi realizada uma tentativa de treinamento do modelo utilizando esta base de dados, mas, devido a limitações de processamento, não foi possível concluir a tarefa, nem mesmo realizando a tentativa de aprendizado por transferência e utilizando o modelo xlm-roberta-base, considerado mais leve que o modelo original.

A aplicação do modelo ClaimDetection baseou-se em sua capacidade de distinguir entre afirmações que são verificáveis e aquelas que não apresentam um conteúdo factual claro. A tarefa principal do modelo foi identificar textos contendo afirmações factuais que exigem confirmação ou validação, ao mesmo tempo em que elimina aqueles que não necessitam de verificação, ajudando assim a filtrar a grande quantidade de informações desnecessárias presentes nas informações de agências de checagem.

O processo de treinamento do modelo SVM (Support Vector Machine) foi realizado com o objetivo de classificar os textos presentes no mesmo database utilizado para o modelo XLM, separando os dados em duas categorias: ``factual'' e ``não factual''. Inicialmente, os textos foram processados e convertidos em embeddings utilizando o modelo XLM-Roberta, um modelo pré-treinado adequado para o processamento de textos em múltiplos idiomas. A partir desses embeddings, um modelo SVM com kernel linear foi treinado para identificar padrões nos textos, classificando-os com base em suas características semânticas extraídas. O treinamento foi realizado usando uma divisão do dataset em conjuntos de treino e teste, sendo 80\% destinados ao conjunto de treinamento e 20\% ao conjunto de teste. A divisão foi estratificada, isto é, a proporção entre as classes factual e não factual foi mantida aproximadamente constante em ambos os subconjuntos. Essa escolha é importante para evitar que o modelo seja treinado ou testado em amostras artificialmente desbalanceadas, o que poderia enviesar as métricas de desempenho. Além disso, foi fixada uma semente de aleatoriedade específica (random\_state=42), garantindo reprodutibilidade: experimentos repetidos com a mesma configuração produzem a mesma partição dos dados.

Como etapa de pré-processamento e representação, os textos foram convertidos para uma forma vetorial numérica por meio da técnica TF-IDF, utilizando-se um vetorizar de texto padrão (TfidfVectorizer). A frequência de cada termo em um texto (TF) é ponderada inversamente pela frequência desse termo no corpus como um todo (IDF), de modo a enfatizar palavras mais informativas e atenuar o peso de termos muito comuns. No presente estudo, foram consideradas não apenas palavras isoladas (unigramas), mas também pares consecutivos de palavras (bigramas), o que permite capturar padrões linguísticos mais ricos, como expressões típicas de linguagem factual (por exemplo, “segundo dados”, “de acordo com”). Além disso, estabeleceu-se um limite máximo de características para o vocabulário e excluíram-se termos extremamente raros, buscando um equilíbrio entre riqueza representacional e controle da dimensionalidade.

Após a vetorização, foi treinado o classificador SVM com função de decisão linear. Utilizou-se um parâmetro de regularização padrão (C=1.0), que controla o compromisso entre complexidade do modelo e capacidade de generalização para encontrar o hiperplano. Para mitigar efeitos de possível desbalanceamento entre as classes factual e não factual, adotou-se um esquema de pesos balanceados entre classes (class\_weight="balanced"), fazendo com que erros na classe minoritária tenham maior peso na função de perda. Tal estratégia é de extrema importância para evitar que o modelo aprenda a favorecer a classe majoritária, o que poderia levar a um desempenho insatisfatório na detecção de textos ``não factual''.
 
Ademais, processamento de linguagem natural (PLN) foi auxiliado pelo NLTK, uma biblioteca que fornece uma ampla gama de recursos para trabalhar e manipular textos, incluindo funções de tokenização, lematização, análise sintática, reconhecimento de entidades, entre outras. Após o pré-processamento com NLTK, os textos foram transformados em vetores numéricos, usados para treinar os modelos para identificação dos termos-chave presentes na base de dados. Além disso, Redes Neurais Recorrentes (RNNs) podem ser utilizadas para análise mais aprofundada do conteúdo textual, identificando padrões sequenciais em textos e oferecendo um método alternativo aos SVMs para tarefas de classificação. Outra abordagem selecionada para o processamento de linguagem natural é o uso de modelos baseados em transformers, como o BERT (Bidirectional Encoder Representations from Transformers). O BERT utiliza o treinamento bidirecional do transformer, permitindo que o modelo tenha um entendimento mais profundo do contexto e do fluxo da linguagem, sendo uma ferramenta promissora para tarefas voltadas à classificação de fake news ~\cite{santos2022}.

\subsection{Classificação Textual}
A classificação dos dados será baseada na intencionalidade por trás da mensagem emitida, destacando as diferenças entre informações deliberadamente falsas ou distorcidas, notícias que buscam intencionalmente manipular e enganar o público, propagandas, publicidade, paródia e sátira. Esses diferentes tipos de conteúdos serão analisados com base em características específicas para distinguir suas intencionalidades e seus impactos na sociedade. Este método para classificação manual segue o mesmo padrão utilizado no trabalho Posicionamento e Desinformação ~\cite{raphaelIC}, já que o objetivo final é gerar uma classificação similar ou mais eficiente do que a realizada no estudo base. Sendo assim, apenas o processo de sumarização e geração de termos-chave se diferencia do projeto original.

Além disso, as notícias filtradas do database de comparação por meio dos termos-chave, obtidas a partir do estudo geral de classificação de desinformação, que também apareceram no presente estudo, mantiveram as classificações previamente atribuídas, evitando retrabalho. Apenas as notícias inéditas foram submetidas a novas classificações manuais.

Importante destacar que o processo de classificação textual não sofreu modificações, tendo como base o relatório técnico Detecção de Posicionamento como Abordagem para Identificação de Conteúdo Desinformativo ~\cite{raphaelIC}, pois o foco do novo estudo é verificar o impacto de detecção de fatos mais otimizadas no processo de classificação textual já realizado no artigo base. Apenas os inputs gerados com os novos termos-chave foram acrescentados no processo de classificação.

\subsection{Análise da eficácia obtida por cada tecnologia}
A avaliação da eficácia dos métodos empregados neste estudo será conduzida por meio de métricas consolidadas na literatura de aprendizado de máquina e processamento de linguagem natural, como precisão, recall (taxa de detecção, capaz de medir a capacidade de identificação das instâncias positivas da base de dados) ~\cite{yacouby2020} e F1-score. Tais métricas possibilitam compreender de maneira abrangente a qualidade das classificações obtidas, uma vez que mensuram não apenas a capacidade de identificar corretamente as instâncias positivas, mas também a proporção de acertos entre todas as predições realizadas (precisão). O F1-score, por sua vez, sintetiza essas duas dimensões em uma única medida harmônica, sendo particularmente relevante em cenários nos quais há desbalanceamento entre classes, como frequentemente ocorre na detecção de desinformação. Dessa forma, a aplicação conjunta dessas métricas permite avaliar de modo robusto a performance dos modelos tanto de aprendizado supervisionado tradicional quanto de aprendizado profundo.

Além das métricas quantitativas, serão realizados testes de eficiência baseados em pesquisas anteriores, permitindo comparar os resultados obtidos com benchmarks já estabelecidos em processos de análise textual em larga escala. Esse tipo de comparação é fundamental para identificar se as tecnologias atualmente utilizadas ainda se mostram adequadas ao combate à disseminação de fake news ou se apresentam sinais de defasagem frente à evolução do fenômeno. A partir dessa análise, será possível apontar fragilidades específicas que demandam aprimoramentos, tais como baixa sensibilidade a variações linguísticas, elevado custo computacional ou dificuldades em lidar com novos padrões de manipulação textual.

Outro aspecto essencial a ser considerado é a etapa de extração de termos-chave, que representa um elo intermediário no processo de checagem de notícias. Nesse ponto, os resultados obtidos pelas diferentes tecnologias serão confrontados com aqueles provenientes do projeto base ``Detecção de Posicionamento como Abordagem para Identificação de Conteúdo Desinformativo'' ~\cite{raphaelIC}. A comparação permitirá verificar a consistência e abrangência dos métodos, identificando quais deles são capazes de recuperar, de forma automática, um número maior de tuítes alinhados ao escopo investigado. Métodos que se aproximarem ou superarem a eficácia do processo manual adotado no projeto de referência serão considerados eficientes. Em contrapartida, técnicas que não demonstrarem desempenho comparável serão classificadas como ineficazes, ainda que apresentem valores satisfatórios em métricas isoladas, uma vez que a utilidade prática está diretamente vinculada à capacidade de identificar conteúdos relevantes em larga escala.

Por fim, a análise de eficácia não se limitará apenas ao desempenho preditivo, mas incluirá também a avaliação do consumo de recursos computacionais, como memória, tempo de processamento e demanda de hardware necessária para o treinamento e a inferência. Esse aspecto é decisivo quando se considera a aplicação das técnicas em cenários reais, especialmente aqueles que exigem monitoramento contínuo e em tempo real, como plataformas de checagem de fatos ou sistemas de vigilância digital. A viabilidade prática dependerá, portanto, da conjugação entre desempenho técnico e eficiência computacional, de modo que um método altamente preciso, mas inviável em termos de tempo e custo, dificilmente poderá ser adotado em larga escala.

Em síntese, a análise da eficácia de cada tecnologia deve integrar diferentes dimensões: métricas de desempenho, capacidade de extração de termos-chave, eficiência na detecção de conteúdos consistentes, consumo de recursos e adaptabilidade frente a novos contextos. Essa perspectiva multidimensional é indispensável para identificar as ferramentas mais promissoras não apenas sob a ótica da precisão técnica, mas também sob a lógica da aplicabilidade prática no combate à disseminação de desinformação em ambientes digitais.

\subsection{Comparação da eficácia entre as tecnologias estudadas durante o projeto}
A avaliação dos resultados obtidos por cada método requer uma análise criteriosa de desempenho, de modo a identificar quais tecnologias apresentam maior potencial de aplicação prática. Para isso, métricas consolidadas no campo do aprendizado de máquina, como precisão, recall e F1-score, serão utilizadas como referência fundamental. A precisão indica a capacidade do modelo de evitar falsos positivos, enquanto o recall mede sua sensibilidade na identificação de instâncias relevantes. Já o F1-score, ao combinar ambas as métricas, permite avaliar de forma equilibrada o desempenho dos classificadores em cenários nos quais tanto a exatidão quanto a abrangência são relevantes. Essas métricas, calculadas a partir das classificações realizadas nos experimentos, oferecem uma base objetiva para a comparação entre diferentes abordagens e servem como critério para a seleção das técnicas mais promissoras.

Entretanto, é importante reconhecer que o desempenho numérico não é o único fator determinante para a escolha do método mais adequado. Modelos baseados em aprendizado profundo, como as arquiteturas de redes neurais recorrentes ou baseadas em transformers, tendem a apresentar resultados superiores em termos de precisão e capacidade de generalização, sobretudo quando aplicados a grandes volumes de dados textuais. Essa vantagem, porém, é frequentemente acompanhada por um custo computacional elevado, demandando maior capacidade de processamento, uso intensivo de memória e tempos de execução prolongados. Tais aspectos podem limitar a aplicabilidade em contextos de larga escala ou em ambientes com restrições de recursos, como em sistemas de checagem automatizada em tempo real.

Por outro lado, métodos mais tradicionais de aprendizado de máquina supervisionado, como os classificadores baseados em SVM ou regressões lineares, embora menos sofisticados em termos de captura de dependências semânticas, apresentam vantagens quanto à eficiência e simplicidade. Seu menor custo computacional permite a implementação em sistemas mais enxutos, além de facilitar a replicabilidade em ambientes distintos sem a necessidade de infraestrutura de alto desempenho. Assim, ainda que possam oferecer resultados inferiores em alguns cenários, esses modelos se destacam pela viabilidade prática, sobretudo quando há limitação de recursos ou quando a demanda de tempo de resposta é um fator crítico.

Além das métricas de desempenho e da análise do custo computacional, outro aspecto essencial a ser considerado é a complexidade de implementação das diferentes técnicas. O ambiente digital, caracterizado pela constante mutação nos processos de fabricação e disseminação de notícias falsas, exige modelos adaptáveis e de fácil atualização. Métodos excessivamente complexos podem não acompanhar a velocidade das mudanças no ecossistema de desinformação, tornando-se rapidamente obsoletos. Por essa razão, a comparação entre os métodos não se restringirá apenas à sua acurácia, mas também levará em conta a flexibilidade de adaptação, a robustez frente a novos padrões linguísticos e a escalabilidade para diferentes contextos.

Outro aspecto relevante a ser considerado é o tempo de processamento exigido por cada abordagem. Métodos tradicionais, como o uso de vetorização TF-IDF combinada a classificadores supervisionados, tendem a apresentar tempos de execução significativamente menores, sendo adequados para cenários em que a rapidez é fator crítico, como em análises em tempo real. Por outro lado, modelos de aprendizado profundo, especialmente aqueles baseados em arquiteturas de transformers, demandam maior capacidade computacional e apresentam tempos de processamento mais elevados devido à complexidade do treinamento e da inferência. Essa diferença impacta diretamente a escalabilidade da solução, visto que aplicações em larga escala, como monitoramento contínuo de redes sociais, exigem um equilíbrio entre acurácia e velocidade de resposta. Dessa forma, a análise comparativa entre os métodos deve considerar não apenas a precisão alcançada, mas também a viabilidade temporal para sua aplicação prática em diferentes contextos.

Portanto, a análise comparativa deve integrar múltiplas dimensões, desempenho, custo computacional, tempo de processamento e complexidade de implementação, com o objetivo de identificar não apenas o modelo mais preciso, mas aquele que seja sustentável, eficiente e aplicável em cenários reais. Essa perspectiva holística é essencial para que a tecnologia desenvolvida seja capaz de responder, de maneira eficaz e contínua, ao desafio dinâmico da detecção e mitigação da desinformação.

\section{Cronograma}

A seguir, é possível observar as etapas de 1 a 10, previstas para a realização deste projeto, que possui uma estimativa de 12 meses para ser completado:

1. Fundamentação teórica

2. Aprendizado em relação a utilização dos softwares

3. Extração e seleção de produções textuais

4. Pré-processamento e mapeamento de textos

5. Registro e obtenção de termos-chave

6. Análise da eficácia obtida por cada tecnologia

7. Comparação da eficácia entre as tecnologias estudadas durante o projeto

8. Integração com dados experimentais

9. Redação dos relatórios

10. Redação de artigo científico